{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch import autograd\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.functional as F\n",
    "import torchvision.datasets as datasets\n",
    "from torchvision import transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import datasets\n",
    "mnist_trainset = datasets.MNIST(root='./data', train=True, download=True, transform=transforms.ToTensor())\n",
    "mnist_testset = datasets.MNIST(root='./data', train=False, download=True, transform=transforms.ToTensor())\n",
    "\n",
    "train_data, train_labels = zip(*mnist_trainset)\n",
    "test_data, test_labels = zip(*mnist_testset)\n",
    "\n",
    "# # split data into minibatches\n",
    "# train_loader = DataLoader(dataset=mnist_trainset, batch_size=batch_size, shuffle=True)\n",
    "# test_loader = DataLoader(dataset=mnist_testset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape before:  torch.Size([1, 28, 28])\n",
      "Shape after:  torch.Size([1, 784])\n"
     ]
    }
   ],
   "source": [
    "# data processing\n",
    "# flatten 28x28 imgs to 784x1 vectors\n",
    "\n",
    "# turn into tensors?\n",
    "flattened_train_data = []\n",
    "\n",
    "print(\"Shape before: \", train_data[0].shape)\n",
    "\n",
    "for i in range(len(train_data)):\n",
    "    flattened_train_data.append(torch.tensor(torch.reshape(train_data[i], (1, 28*28))))   \n",
    "\n",
    "print(\"Shape after: \", flattened_train_data[0].shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape before:  torch.Size([1, 28, 28])\n",
      "Shape after:  torch.Size([1, 784])\n"
     ]
    }
   ],
   "source": [
    "# flatten test data\n",
    "\n",
    "flattened_test_data = []\n",
    "\n",
    "print(\"Shape before: \", test_data[0].shape)\n",
    "\n",
    "for i in range(len(test_data)):\n",
    "    flattened_test_data.append(torch.tensor(torch.reshape(test_data[i], (1, 28*28))))   \n",
    "\n",
    "print(\"Shape after: \", flattened_test_data[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000 10100\n",
      "100\n",
      "100\n"
     ]
    }
   ],
   "source": [
    "# given batch number from, return batch of data\n",
    "def train_batchify(batch_num, batch_size):\n",
    "    start_index = batch_num * batch_size\n",
    "    data = flattened_train_data[start_index:start_index + batch_size]\n",
    "    labels = train_labels[start_index:start_index + batch_size]  \n",
    "    \n",
    "    print(start_index, start_index + batch_size)\n",
    "    \n",
    "    return (labels, data)\n",
    "\n",
    "labels, data = train_batchify(100, 100)\n",
    "print(labels.__len__())\n",
    "print(data.__len__())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # given batch number from, return batch of data\n",
    "# def test_batchify(batch_num, batch_size):\n",
    "#     start_index = batch_num * batch_size\n",
    "#     data = flattened_test_data[start_index:start_index + batch_size]\n",
    "#     labels = test_labels[start_index:start_index + batch_size]  \n",
    "    \n",
    "#     print(start_index, start_index + batch_size)\n",
    "    \n",
    "#     return (labels, data)\n",
    "\n",
    "# labels, data = test_batchify(100, 100)\n",
    "# print(labels.__len__())\n",
    "# print(data.__len__())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameters\n",
    "input_size = 784\n",
    "num_classes = 10\n",
    "\n",
    "# define the model\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        # 1 layer\n",
    "        self.layer_one = nn.Linear(input_size, num_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.softmax(x)\n",
    "        return x\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train model\n",
    "classifier = Net()\n",
    "optimizer = optim.SGD(classifier.parameters(), lr=0.01, momentum=0.5)\n",
    "\n",
    "loss_history = []\n",
    "accuracy_history = []\n",
    "\n",
    "def train(epoch):\n",
    "    classifier.train() # training mode\n",
    "    \n",
    "    batch_size = 1000\n",
    "    max_batch_num = len(train_labels) // batch_size\n",
    "    \n",
    "    for batch_num in range(0, max_batch_num + 1):\n",
    "        labels, data = train_batchify(batch_num, batch_size)\n",
    "        \n",
    "        labels = Variable(labels)\n",
    "        data = Variable(data)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        predictions = classifier(data)\n",
    "        loss = F.nll_loss(predictions, labels)\n",
    "        loss.backward()\n",
    "        \n",
    "        loss_history.append(loss.data[0])\n",
    "        optimizer.step()\n",
    "        \n",
    "        if batch_id == max_batch_num:\n",
    "            print(loss.data[0])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:  0\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'batch_size' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-26-dfb91492ea26>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"epoch: \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m     \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m     \u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-19-12aeb8be05b0>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(epoch)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mBATCH_SIZE\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1000\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m     \u001b[0mmax_batch_num\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_labels\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m//\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mbatch_num\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_batch_num\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'batch_size' is not defined"
     ]
    }
   ],
   "source": [
    "def test(epoch):\n",
    "    classifier.eval() # test mode\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    \n",
    "    for i in range(len(test_labels)):\n",
    "        data = Variable(flattened_test_data[i], volatile=True)\n",
    "        data = Variable(test_label[i])\n",
    "        \n",
    "        output = classifier(data)\n",
    "        test_loss += F.nll_loss(output, target).data[0]\n",
    "        prediction = output.data.max(1)[1] # index of max-log probability\n",
    "        correct += preddiction.eq(data).cpu().sum()\n",
    "        \n",
    "    test_loss = test_loss\n",
    "    test_loss /= len(test_labels)\n",
    "    acc_history.append(accuracy)\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        test_loss, correct, len(test_labels),accuracy))\n",
    "    \n",
    "\n",
    "for epoch in range(0, 3):\n",
    "    print(\"epoch: \", epoch)\n",
    "    train(epoch)\n",
    "    test(epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
